{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "자연어 처리 - 텐서플로우로 텍스트 분류",
      "provenance": [],
      "collapsed_sections": [
        "DfQa9C57WgLU",
        "fd88o7UiaAHe",
        "uCPRPOpKXIi6",
        "3j7XpKstnofK",
        "YAVd5yj5GHoV",
        "xsQsnX2qaIed",
        "CDp85Q-WaPv5",
        "BQzV6g9taUBn",
        "-nNauczPafDd",
        "TxLjeLVGandD",
        "uIHdxvRjasz0",
        "sKLXPe_5HHxv",
        "nmpzmV3GbdWW",
        "F3KXtyXEPzKY",
        "Z6VjFTfhbqAh",
        "uWRfQZYBcOZs",
        "oRcpq9W_btK6",
        "1_6uJKbDbvFX",
        "b_WG19Csb0jr",
        "4PKzXx0Jb2cC",
        "ei-BBq8KcLGZ",
        "Pg7Ab60acPUH",
        "J4YnppLscTBO",
        "_3Mrj1VzcWDy",
        "nY9HyBJG2-MG",
        "Dc1LDhZ23CC-",
        "UOFSVtmd3HH2",
        "OAXrZQPy3Q27",
        "BHWy2wlG3UaF",
        "uxBBY_JPOLZ7"
      ],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/moey920/Object-Detection/blob/master/%EC%9E%90%EC%97%B0%EC%96%B4_%EC%B2%98%EB%A6%AC_%ED%85%90%EC%84%9C%ED%94%8C%EB%A1%9C%EC%9A%B0%EB%A1%9C_%ED%85%8D%EC%8A%A4%ED%8A%B8_%EB%B6%84%EB%A5%98.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ngD1Q34lCa5C",
        "colab_type": "text"
      },
      "source": [
        "# 한글 텍스트 분류 튜토리얼\n",
        "\n",
        "## 소개\n",
        "\n",
        "이번 시간에는 한글 텍스트를 가지고 감정분류를 하는 딥러닝 모델을 구현해보도록 하겠습니다.\n",
        "\n",
        "이 튜토리얼은 텐서플로 Estimator와 네이버 영화평점 데이터셋을 활용하여 딥러닝 모델로 어떻게 감정분류를 하는지 봅니다. \n",
        "\n",
        "튜토리얼 시작에 앞서서 어떻게 진행하는지에 대해서 아래 목차를 확인하고 가도록 합시다.\n",
        "\n",
        "1. 데이터 불러오기 - 네이버 영화평점 데이터셋을 어떻게 불러와 다루는지 보도록 합니다.\n",
        "\n",
        "2. 데이터 분석\n",
        "\n",
        "  2.1 리뷰 길이 분포 - 전체 데이터 길이가 어떠한 분포로 되어있는지 확인해보도록 합니다.\n",
        "  \n",
        "  2.2 데이터 라벨 분포 - 데이터 학습 라벨 분포가 어떻게 되어있는지 확인해보도록 합니다.\n",
        "  \n",
        "  2.3 워드클라우드 - 데이터에 가장 빈번하게 등장하는 단어들이 어떤건지 확인해보도록 합니다.\n",
        "  \n",
        "  2.4 기타 - 그 외에 어떠한 데이터를 알아 볼 수 있는지 봅시다.\n",
        "  \n",
        "3. 전처리\n",
        "\n",
        "  3.1 형태소 분석 - 형태소 분석을 통해 한글 토크나이징을 어떻게 하는지 보도록 합니다. \n",
        "  \n",
        "  3.2 불용어 처리 - 어떤 단어를 불용어라 보는지 확인하고 어떻게 데이터에서 제거하는지 보도록 합니다.\n",
        "  \n",
        "  3.3 인덱싱 - 모델에 학습하기 위해 단어 사전을 어떻게 구성하는지 보고 텍스트를 어떻게 인덱싱하는지 보도록 합니다.\n",
        "  \n",
        "4. 모델 학습 - 모델링은 tf.Estimator를 통해 어떻게 데이터를 입력하고 모델 학습을 하는지 보도록 합니다.\n",
        "\n",
        "  4.1 데이터 입력 함수 - 모델 학습과 평가를 위해 어떻게 데이터를 구성하는지 알아보도록 합니다.\n",
        "  \n",
        "  4.2 모델 함수 - CNN 모델을 어떻게 만드는지 보고 이 모델로 어떻게 텍스트 분류기를 만드는지 보도록 합니다.\n",
        "  \n",
        "  4.3 학습 및 평가 - tf.Estimator를 가지고 어떻게 모델 학습과 평가를 하는지 보도록 합니다.\n",
        "  \n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t4gNE48gJXwK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!pip install konlpy"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Rue1mwk_sAM2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# 나눔고딕 설치\n",
        "!apt -qq -y install fonts-nanum > /dev/null\n",
        "#!pip install fonts-nanum > /dev/null\n",
        "import matplotlib.font_manager as fm\n",
        "fontpath = '/usr/share/fonts/truetype/nanum/NanumBarunGothic.ttf'\n",
        "font = fm.FontProperties(fname=fontpath, size=9)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_RKkoq0LW0fV",
        "colab_type": "text"
      },
      "source": [
        "## 시작에 앞서...\n",
        "\n",
        "아마 튜토리얼을 처음 시작 하시는 분은 텐서플로우를 거의 다뤄보시지 않은 분들일 겁니다. \n",
        "\n",
        "여기서는 딥러닝 모델링을 위해 tf.data, tf.estimator 위주로 주로 다루게 되는데요.\n",
        "\n",
        "이와 관련해서 대략 tf.data와 tf.estimator가 어떤것인지 알 수 있는 실습 페이지를 두었습니다.\n",
        "\n",
        "아래 링크를 확인해주시고 각 모듈들이 어떻게 동작하는지를 이해해보았으면 좋겠습니다.\n",
        "\n",
        "- tf.data 링크: https://bit.ly/2Uuliqa \n",
        "\n",
        "- tf.estimator 링크: https://bit.ly/2sWmnLJ \n",
        "\n",
        "\n",
        "### Estimator 도식화\n",
        "![estimator](https://t1.daumcdn.net/cfile/tistory/9910C53359AF8CA334)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IQoQoJ6CBPiJ",
        "colab_type": "text"
      },
      "source": [
        "## 라이브러리 불러오기\n",
        "\n",
        "여러분들이 텍스트 분류기를 만들기 위해서는 다음과 같은 라이브러리들이 필요합니다.\n",
        "\n",
        "- numpy: 벡터 연산 및 여러 수학 연산에서 많이 사용합니다.\n",
        "\n",
        "- pandas: raw 테이블 데이터를 입력받고 처리하는데 사용합니다.\n",
        "\n",
        "- matplotlib: 데이터 분석 시에 통계를 보는데 사용합니다.\n",
        "\n",
        "- wordcloud: 단어 분포를 보는데 사용합니다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z1t16WRnljEh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np \n",
        "import pandas as pd\n",
        "import os\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from wordcloud import WordCloud\n",
        "%matplotlib inline"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hF4xMXBRSzkT",
        "colab_type": "text"
      },
      "source": [
        "## 데이터 불러오기\n",
        "\n",
        "판다스 라이브러리를 통해 텍스트 데이터를 불러오도록 합니다.\n",
        "\n",
        "(현재 텍스트 데이터는 깃헙에 저장된 csv 파일을 불러오는겁니다.)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N9Wt9Mll70lc",
        "colab_type": "code",
        "outputId": "bbd53fc4-a324-4c03-ed4e-7cc678f56b6e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 127
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ou_lvRSnkwL8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#path = '/content/drive/My Drive/text/dataset_utf8.csv'\n",
        "#with open(path, 'r', encoding='utf-8') as f:\n",
        "#    train_file_link  = f.readlines()\n",
        "train_file_link = '/content/drive/My Drive/text/dataset.txt'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s80r58sDTr-R",
        "colab_type": "text"
      },
      "source": [
        "불러온 데이터를 보면 id, document, label로 구분이 되어있습니다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mUpNx7rRljHt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_data = pd.read_csv(train_file_link, header = 0, delimiter = '\\t', quoting = 3)\n",
        "train_data.head(10)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4SNH2oNZTm99",
        "colab_type": "text"
      },
      "source": [
        "## 데이터 분석\n",
        "\n",
        "불러온 텍스트 데이터를 이제 하나하나 분석을 해보며 이 데이터는 어떻게 구성이 되어있는지 알아가보도록 합시다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CbLJ4YqpUJsG",
        "colab_type": "text"
      },
      "source": [
        "### 학습 데이터 갯수"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JV95w8itljKi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print('전체 학습데이터의 개수: {}'.format(len(train_data)))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LBcqIhmoUNVd",
        "colab_type": "text"
      },
      "source": [
        "### 텍스트 길이 분포\n",
        "\n",
        "텍스트 분류 모델링을 하는 경우 전체 학습 데이터 길이가 어떻게 되는지 아는게 중요할 때가 있습니다. \n",
        "\n",
        "여기서는 Character 단위 Word 단위 2가지로 나누어 분포를 확인해보고자 합니다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b0GjniaxB99o",
        "colab_type": "code",
        "outputId": "14a132c3-c8e1-41a8-89ec-2ad81cffb9b6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "print('id' in train_data.columns)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "True\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "501H0cKNVEI1",
        "colab_type": "text"
      },
      "source": [
        "#### Character 단위 텍스트 길이 분포"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6rZ9GMkZljNH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_lenght = train_data['document'].astype(str).apply(len)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VQ8fBBjmljPo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# 그래프에 대한 이미지 사이즈 선언\n",
        "# figsize: (가로, 세로) 형태의 튜플로 입력\n",
        "plt.figure(figsize=(12, 5))\n",
        "# 히스토그램 선언\n",
        "# bins: 히스토그램 값들에 대한 버켓 범위\n",
        "# range: x축 값의 범위\n",
        "# alpha: 그래프 색상 투명도\n",
        "# color: 그래프 색상\n",
        "# label: 그래프에 대한 라벨\n",
        "plt.hist(train_lenght, bins=200, alpha=0.5, color= 'r', label='word')\n",
        "# plt.yscale('log', nonposy='clip')\n",
        "# 그래프 제목\n",
        "plt.title('Text length Distribution')\n",
        "# 그래프 x 축 라벨\n",
        "plt.xlabel('Length of sentense')\n",
        "# 그래프 y 축 라벨\n",
        "plt.ylabel('Number of sentense')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AcGWyFZJljR6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print('문장 길이 최대 값: {}'.format(np.max(train_lenght)))\n",
        "print('문징 길이 최소 값: {}'.format(np.min(train_lenght)))\n",
        "print('문장 길이 평균 값: {:.2f}'.format(np.mean(train_lenght)))\n",
        "print('문장 길이 표준편차: {:.2f}'.format(np.std(train_lenght)))\n",
        "print('문장 길이 중간 값: {}'.format(np.median(train_lenght)))\n",
        "# 사분위의 대한 경우는 0~100 스케일로 되어있음\n",
        "print('문장 길이 제 1 사분위: {}'.format(np.percentile(train_lenght, 25)))\n",
        "print('문장 길이 제 3 사분위: {}'.format(np.percentile(train_lenght, 75)))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IKvQ5JrTWQKk",
        "colab_type": "text"
      },
      "source": [
        "#### Word 단위 텍스트 길이 분포"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2EGtgvAeljYz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_word_counts = train_data['document'].astype(str).apply(lambda x:len(x.split(' ')))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_fMyi4qNljbB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "plt.figure(figsize=(12, 5))\n",
        "plt.hist(train_word_counts, bins=50, facecolor='r',label='train')\n",
        "plt.title('Log-Histogram of word count in review', fontsize=15)\n",
        "# plt.yscale('log', nonposy='clip')\n",
        "plt.legend()\n",
        "plt.xlabel('Number of words', fontsize=15)\n",
        "plt.ylabel('Number of sentense', fontsize=15)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zc6G5cG7ljhd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print('문장 단어 개수 최대 값: {}'.format(np.max(train_word_counts)))\n",
        "print('문장 단어 개수 최소 값: {}'.format(np.min(train_word_counts)))\n",
        "print('문장 단어 개수 평균 값: {:.2f}'.format(np.mean(train_word_counts)))\n",
        "print('문장 단어 개수 표준편차: {:.2f}'.format(np.std(train_word_counts)))\n",
        "print('문장 단어 개수 중간 값: {}'.format(np.median(train_word_counts)))\n",
        "# 사분위의 대한 경우는 0~100 스케일로 되어있음\n",
        "print('문장 단어 개수 제 1 사분위: {}'.format(np.percentile(train_word_counts, 25)))\n",
        "print('문장 단어 개수 제 3 사분위: {}'.format(np.percentile(train_word_counts, 75)))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DfQa9C57WgLU",
        "colab_type": "text"
      },
      "source": [
        "### 데이터 라벨 분포\n",
        "\n",
        "이 영수증 텍스트 데이터 라벨은 15 종류로 되어 있습니다.\n",
        "\n",
        "1. 브랜드명, 지점명\n",
        "2. 주소\n",
        "3. 사업자등록번호, 지점대표, 전화번호\n",
        "4. 구매일시\n",
        "5. 상품명\n",
        "6. 상품바코드, 단가, 수량, 금액\n",
        "7. 결제금액\n",
        "8. 카드금액\n",
        "9. 카드번호\n",
        "10. 카드사명\n",
        "11. 카드승인번호\n",
        "12. 영수증 바코드\n",
        "13. 결제방법\n",
        "14. 현금영수증 승인번호  \n",
        "0: 불필요 \n",
        "로 분류했습니다.\n",
        "\n",
        "\n",
        "각 데이터 라벨의 분포가 어떠한지 살펴봅시다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BRl3JsOWljUW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "fig, axe = plt.subplots(ncols=1)\n",
        "fig.set_size_inches(6, 3)\n",
        "sns.countplot(train_data['label'])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TiwnXkFNljWp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print(\"긍정 리뷰 개수: {}\".format(train_data['label'].value_counts()[1]))\n",
        "print(\"부정 리뷰 개수: {}\".format(train_data['label'].value_counts()[0]))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fd88o7UiaAHe",
        "colab_type": "text"
      },
      "source": [
        "### 워드클라우드\n",
        "\n",
        "과연 이 데이터에는 어떤 말들이 많이 쓰이고 있을까요?\n",
        "\n",
        "워드클라우드는 데이터에 어떤 단어들이 많이 분포되어있는지 바로 알 수 있게 해줍니다.\n",
        "\n",
        "워드클라우드를 통해서 어떤 단어가 가장 많이 쓰이는지 살펴보도록 합시다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eiXd0ARCZiqK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_review = [review for review in train_data['document'] if type(review) is str]\n",
        "wordcloud = WordCloud(font_path=fontpath).generate(' ' .join(train_review))\n",
        "plt.figure(figsize = (15 , 10))\n",
        "plt.imshow(wordcloud, interpolation='bilinear') \n",
        "plt.axis('off')\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uCPRPOpKXIi6",
        "colab_type": "text"
      },
      "source": [
        "### 기타\n",
        "\n",
        "텍스트 문장에는 종종 문장의 끝의 기호가 마침표냐 물음표냐 등에 따라 텍스트가 어떤 의미를 가졌는지 대략 짐작을 할 때도 있습니다.\n",
        "\n",
        "이 텍스트 데이터셋 같은 경우는 어떤지 한 번 보도록 합시다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eNwfE-V0ljjx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "qmarks = np.mean(train_data['document'].astype(str).apply(lambda x: '?' in x)) # 물음표가 구두점으로 쓰임\n",
        "fullstop = np.mean(train_data['document'].astype(str).apply(lambda x: '.' in x)) # 마침표\n",
        "                  \n",
        "print('물음표가 있는 리뷰: {:.2f}%'.format(qmarks * 100))\n",
        "print('마침표가 있는 리뷰: {:.2f}%'.format(fullstop * 100))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t6OTA0xiYzch",
        "colab_type": "text"
      },
      "source": [
        "## 데이터 전처리\n",
        "\n",
        "텍스트 분류기 모델 학습을 위해 데이터를 모델에 학습시킬 수 있도록 구성해주어야 합니다.\n",
        "\n",
        "여기서는 텍스트 필터링과 토크나이징, 불용어처리 등을 다루고, 인덱싱처리를 통해 모델에 들어 갈 입력 데이터를 구성합니다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3j7XpKstnofK",
        "colab_type": "text"
      },
      "source": [
        "### 잠깐! 용어 정리\n",
        "\n",
        "- 배치(batch): 모델 학습에 한 번에 입력할 데이터셋\n",
        "\n",
        "- 에폭(epoch): 모델 학습시 전체 데이터를 학습한 횟 수\n",
        "\n",
        "- 스텝(step): (모델 학습의 경우) 하나의 배치를 학습한 횟 수\n",
        "\n",
        "- 토큰(token): (여기서는) 문장 또는 문단의 기본 구성 단위 (예를 들어 단위, 형태소)\n",
        "\n",
        "- 토크나이징(tokenizing): 문단 또는 문장 문자열을 하나의 토큰 단위로 쪼개는 작업\n",
        "\n",
        "- 인덱싱(indexing): 토큰 문자열을 숫자(인덱스)로 변환하여 표현하는 작업\n",
        "\n",
        "- 사전(vocabulary): 토큰 문자열과 인덱스의 관계를 정의해둔 셋\n",
        "\n",
        "- n-그램(n-gram): 입력한 문자열을 N개의 기준 단위로 절단하는 방법"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YAVd5yj5GHoV",
        "colab_type": "text"
      },
      "source": [
        "### 전처리를 위한 라이브러리\n",
        "\n",
        "- konlpy: 형태소 단위로 토크나이징을 하기 위해 필요한 라이브러리 입니다.\n",
        "- tensorflow.python.keras.preprocessing: 단어사전을 만들고 인덱싱을 위해 필요한 모듈입니다.\n",
        "- tqdm: Progress Bar 라이브러리 입니다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8Pdxw4YUljmK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import re\n",
        "import json\n",
        "from konlpy.tag import Okt\n",
        "from tensorflow.python.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.python.keras.preprocessing.text import Tokenizer\n",
        "\n",
        "from tqdm import tqdm"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f8JPM2LfKjfe",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_data.head(10)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xsQsnX2qaIed",
        "colab_type": "text"
      },
      "source": [
        "### RegEx를 활용한 텍스트 필터링\n",
        "\n",
        "모델에서 다뤄야 할 단어가 너무 많아도 문제가 되는 경우가 있습니다.\n",
        "\n",
        "이를 제어하기 위해 한글 데이터만을 받도록 regex를 통해 처리합니다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fkRAHn3fJ6UC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "review_text = re.sub(\"[^가-힣ㄱ-ㅎㅏ-ㅣ\\\\s]\", \"\", train_data['document'][0]) \n",
        "print(review_text)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CDp85Q-WaPv5",
        "colab_type": "text"
      },
      "source": [
        "### KoNLPy를 활용한 토크나이징\n",
        "\n",
        "한글은 문장에서 단어들로 정의하여 분리하는게 쉽지 않습니다.\n",
        "\n",
        "공백단위로 쪼개어 단어로 분리할 수 있기도 하지만 여기서는 형태소 분석기를 활용하여 단어로 분리해보도록 하겠습니다.\n",
        "\n",
        "KoNLPy를 활용하여 토크나이징 하는 방법을 알아봅시다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V3IucC4MJ6ea",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "okt=Okt()\n",
        "review_text = okt.morphs(review_text, stem=True)\n",
        "print(review_text)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BQzV6g9taUBn",
        "colab_type": "text"
      },
      "source": [
        "### 불용어 사전을 활용한 텍스트 필터링"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_0gdS6sIJ6l1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "stop_words = set(['은', '는', '이', '가', '하', '아', '것', '들','의', '있', '되', '수', '보', '주', '등', '한'])\n",
        "clean_review = [token for token in review_text if not token in stop_words]\n",
        "print(clean_review)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-nNauczPafDd",
        "colab_type": "text"
      },
      "source": [
        "### 위 세 과정을 세트로 하면..."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GqmzD-yJJ6t4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def preprocessing(review, okt, remove_stopwords = False, stop_words = []):\n",
        "    # 함수의 인자는 다음과 같다.\n",
        "    # review : 전처리할 텍스트\n",
        "    # okt : okt 객체를 반복적으로 생성하지 않고 미리 생성후 인자로 받는다.\n",
        "    # remove_stopword : 불용어를 제거할지 선택 기본값은 False\n",
        "    # stop_word : 불용어 사전은 사용자가 직접 입력해야함 기본값은 비어있는 리스트\n",
        "    \n",
        "    # 1. 한글 및 공백을 제외한 문자 모두 제거.\n",
        "    review_text = re.sub(\"[^가-힣ㄱ-ㅎㅏ-ㅣ\\\\s]\", \"\", review)\n",
        "    \n",
        "    # 2. okt 객체를 활용해서 형태소 단위로 나눈다.\n",
        "    word_review = okt.morphs(review_text, stem=True)\n",
        "    \n",
        "    if remove_stopwords:\n",
        "        \n",
        "        # 불용어 제거(선택적)\n",
        "        word_review = [token for token in word_review if not token in stop_words]\n",
        "        \n",
        "   \n",
        "    return word_review"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZV74mUyLKtPS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "stop_words = [ '은', '는', '이', '가', '하', '아', '것', '들','의', '있', '되', '수', '보', '주', '등', '한']\n",
        "okt = Okt()\n",
        "clean_train_review = []\n",
        "\n",
        "for review in tqdm(train_data['document']):\n",
        "    # 비어있는 데이터에서 멈추지 않도록 string인 경우만 진행\n",
        "    if type(review) == str:\n",
        "        clean_train_review.append(preprocessing(review, okt, remove_stopwords = True, stop_words=stop_words))\n",
        "    else:\n",
        "        clean_train_review.append([])  #string이 아니면 비어있는 값 추가"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TxLjeLVGandD",
        "colab_type": "text"
      },
      "source": [
        "### 단어 사전 생성 및 인덱싱\n",
        "\n",
        "이제 텍스트 데이터를 딥러닝 모델에 입력할 수 있도록 만들어 봅시다.\n",
        "\n",
        "여기서는 텐서플로의 Tokenizer를 통해 단어 인덱싱 작업을 합니다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YiGbtlcMMdqv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "tokenizer = Tokenizer()\n",
        "tokenizer.fit_on_texts(clean_train_review)\n",
        "train_sequences = tokenizer.texts_to_sequences(clean_train_review)\n",
        "train_labels = np.array(train_data['label']) # 학습 데이터의 라벨"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uIHdxvRjasz0",
        "colab_type": "text"
      },
      "source": [
        "### 패딩 작업\n",
        "\n",
        "여러분이 만약에 모델학습을 하게된다면 학습데이터를 한 문장씩 입력하여 학습하지 않습니다. \n",
        "여러 문장을 한번에 묶어서 모델에 입력하게 되는데 이 입력하는 데이터 입력 셋 단위를 '배치'라 합니다.\n",
        "\n",
        "여러 문장 데이터를 하나의 배치로 구성하게 되면 각 문장의 길이가 각각 다르게 되어 있는걸 확인 하실 수 있습니다.\n",
        "하지만, 각 문장 입력 데이터 길이가 다르다면 모델 연산을 하는데 문제가 생깁니다. \n",
        "왜냐하면 모델은 입력 사이즈가 고정되어 연산되는 걸 선호하기 때문에 그렇습니다.\n",
        "\n",
        "이러한 문제 때문에 가급적 고정된 문장 길이를 정해두고 입력 문장의 길이를 맞추는 것이 가장 좋습니다.\n",
        "문장이 긴 경우에는 고정된 문장 길이만큼 자르면 되지만 짧은 경우에는 그렇지 않습니다.\n",
        "패딩 작업은 이러한 짧은 문장에 대해서 '<PAD\\>'와 같은 토큰을 길이를 맞추는 작업입니다.\n",
        "\n",
        "여기서는 간단하게 pad_sequences 함수가 고정된 길이 만큼의 각 문장 데이터 길이를 맞춰줍니다.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QgY0yGxXMmZr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "MAX_SEQUENCE_LENGTH = 8 # 문장 최대 길이\n",
        "\n",
        "train_inputs = pad_sequences(train_sequences, maxlen=MAX_SEQUENCE_LENGTH, padding='post') # 학습 데이터를 벡터화"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gVh4zEQKcI0J",
        "colab_type": "text"
      },
      "source": [
        "## 모델 학습을 위한 준비"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sKLXPe_5HHxv",
        "colab_type": "text"
      },
      "source": [
        "### 모델링을 위한 라이브러리\n",
        "\n",
        "- tensorflow: 딥러닝 모델링을 위한 라이브러리\n",
        "- sklearn.model_selection.train_test_split: 데이터를 학습용과 평가용으로 나누기 위한 함수"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mqRKrJpdN7U3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import os\n",
        "import tensorflow as tf\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "tf.VERSION"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kkzxWTKlN7aM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "input_data = train_inputs\n",
        "label_data = train_labels\n",
        "word_vocab = tokenizer.word_index"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nmpzmV3GbdWW",
        "colab_type": "text"
      },
      "source": [
        "### 하이퍼파라메터 설정\n",
        "\n",
        "아래 상수들은 모델에 필요한 하이퍼파라메터 값들입니다.\n",
        "\n",
        "하이퍼파라메터는 다음과 같이 구성되어 있습니다.\n",
        "\n",
        "- TEST_SPLIT : 전체 데이터 중 얼마나 평가 데이터로 활용할 지 정하는 상수(0~1)\n",
        "\n",
        "- RNG_SEED : 데이터 분리를 하는데 랜덤 생성 시드 번호를 고정해두는 상수\n",
        "\n",
        "- VOCAB_SIZE : 단어 임베딩 총 단어수에 대한 상수\n",
        "\n",
        "- EMB_SIZE : 단어 임베딩 및 CNN 네트워크 출력 임베딩 차원에 대한 상수\n",
        "\n",
        "- BATCH_SIZE : 학습 시 모델에 한 번 입력할 데이터 양, 배치 사이즈 상수\n",
        "\n",
        "- NUM_EPOCHS: 전체 데이터를 몇 번 학습 시킬 것인지 정하는(에포크)) 상수"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oNif1WCXN7el",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "TEST_SPLIT = 0.1\n",
        "RNG_SEED = 13371447\n",
        "VOCAB_SIZE = len(word_vocab) + 1\n",
        "EMB_SIZE = 128\n",
        "BATCH_SIZE = 16\n",
        "NUM_EPOCHS = 1"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F3KXtyXEPzKY",
        "colab_type": "text"
      },
      "source": [
        "### 학습 및 평가 데이터 분리\n",
        "\n",
        "데이터를 학습과 평가 데이터로 구분해줍니다.\n",
        "\n",
        "여기서는 10%의 데이터만 평가 데이터로 사용합니다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gwrA1cRTP0Ft",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "input_train, input_eval, label_train, label_eval = train_test_split(input_data, label_data, test_size=TEST_SPLIT, random_state=RNG_SEED)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z6VjFTfhbqAh",
        "colab_type": "text"
      },
      "source": [
        "### 데이터 입력 함수\n",
        "\n",
        "데이터를 모델에 입력할 준비를 해봅시다.                                                                                                                   \n",
        "\n",
        "tf.estimator를 활용하여 모델링을 하기 위해선 데이터 입력 함수를 선언해줘야 합니다.\n",
        "\n",
        "데이터 입력 함수 내부는 전부 tf.data.Datset으로 다루어져 있습니다.\n",
        "\n",
        "앞에서 tf.data를 이미 보셨다면 구현 내용은 쉽게 이해하실 수 있을겁니다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xk6yk1TwN7oh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def mapping_fn(X, Y):\n",
        "    input, label = {'x': X}, Y\n",
        "    return input, label\n",
        "\n",
        "def train_input_fn():\n",
        "    dataset = tf.data.Dataset.from_tensor_slices((input_train, label_train))\n",
        "    dataset = dataset.shuffle(buffer_size=len(input_train))\n",
        "    dataset = dataset.batch(BATCH_SIZE)\n",
        "    dataset = dataset.map(mapping_fn)\n",
        "    dataset = dataset.repeat(count=NUM_EPOCHS)\n",
        "    iterator = dataset.make_one_shot_iterator()\n",
        "    \n",
        "    return iterator.get_next()\n",
        "\n",
        "def eval_input_fn():\n",
        "    dataset = tf.data.Dataset.from_tensor_slices((input_eval, label_eval))\n",
        "    dataset = dataset.batch(128)\n",
        "    dataset = dataset.map(mapping_fn)\n",
        "    iterator = dataset.make_one_shot_iterator()\n",
        "    \n",
        "    return iterator.get_next()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nlNy-nOYMxJ7",
        "colab_type": "text"
      },
      "source": [
        "## 모델 학습\n",
        "\n",
        "이제 모델링을 직접 해보도록 하겠습니다.\n",
        "\n",
        "두 개의 모델을 소개하는데 첫 번째 모델은 Convolutional Neural Network를 활용한 분류 모델이고 \n",
        "\n",
        "두 번째 모델은 Recurrent Neural Network를 활용한 분류 모델입니다.\n",
        "\n",
        "(참고로 모델은 실제 논문에 나온 모델을 간소화해둔 모델입니다.)\n",
        "\n",
        "모델을 간단하게 이해하고 본격적으로 모델 학습을 해보도록 합니다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uWRfQZYBcOZs",
        "colab_type": "text"
      },
      "source": [
        "### Convolutional Neural Network (CNN) 모델\n",
        "\n",
        "![cnn_model](http://www.wildml.com/wp-content/uploads/2015/11/Screen-Shot-2015-11-06-at-8.03.47-AM.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oRcpq9W_btK6",
        "colab_type": "text"
      },
      "source": [
        "### 모델 함수\n",
        "\n",
        "이제 tf.estimator에 등록할 모델 함수를 구현해 봅시다.\n",
        "\n",
        "여기서는 CNN 분류 모델을 간소하게 만든 모델입니다.\n",
        "\n",
        "처음 하시는 분은 CNN 분류를 텐서플로우로 어떻게 구현하는지 보시기 바랍니다.\n",
        "\n",
        "이미 다뤄보신 분이라면 어떻게 하면 보다 성능이 좋아질 수 있을지 고민해보시면 좋을 것 같습니다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FPGsJnDmN7tp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def cnn_model_fn(features, labels, mode, params):\n",
        "    TRAIN = mode == tf.estimator.ModeKeys.TRAIN\n",
        "    EVAL = mode == tf.estimator.ModeKeys.EVAL\n",
        "    PREDICT = mode == tf.estimator.ModeKeys.PREDICT\n",
        "\n",
        "    embedding_layer = tf.keras.layers.Embedding(\n",
        "                    VOCAB_SIZE,\n",
        "                    EMB_SIZE)(features['x'])\n",
        "\n",
        "    dropout_emb = tf.keras.layers.Dropout(rate = 0.2)(embedding_layer)\n",
        "    \n",
        "    conv = tf.keras.layers.Conv1D(filters=32, kernel_size=3, padding='same', \n",
        "                                  activation=tf.nn.relu)(dropout_emb)\n",
        "    \n",
        "  \n",
        "    pool = tf.keras.layers.GlobalMaxPool1D()(conv)\n",
        "\n",
        "    hidden = tf.keras.layers.Dense(units=250, activation=tf.nn.relu)(pool)   \n",
        "\n",
        "\n",
        "    dropout_hidden = tf.keras.layers.Dropout(rate=0.2)(hidden, training = TRAIN)\n",
        "    logits = tf.keras.layers.Dense(units=1)(dropout_hidden)\n",
        "\n",
        "    if labels is not None:\n",
        "        labels = tf.reshape(labels, [-1, 1])\n",
        "        \n",
        "    if TRAIN:\n",
        "        global_step = tf.train.get_global_step()\n",
        "        loss = tf.losses.sigmoid_cross_entropy(labels, logits)\n",
        "        train_op = tf.train.AdamOptimizer(0.001).minimize(loss, global_step)\n",
        "\n",
        "        return tf.estimator.EstimatorSpec(mode=mode, train_op=train_op, loss = loss)\n",
        "    \n",
        "    elif EVAL:\n",
        "        loss = tf.losses.sigmoid_cross_entropy(labels, logits)\n",
        "        pred = tf.nn.sigmoid(logits)\n",
        "        accuracy = tf.metrics.accuracy(labels, tf.round(pred))\n",
        "        return tf.estimator.EstimatorSpec(mode=mode, loss=loss, eval_metric_ops={'acc': accuracy})\n",
        "        \n",
        "    elif PREDICT:\n",
        "        return tf.estimator.EstimatorSpec(\n",
        "            mode=mode,\n",
        "            predictions={\n",
        "                'prob': tf.nn.sigmoid(logits),\n",
        "            }\n",
        "        )"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1_6uJKbDbvFX",
        "colab_type": "text"
      },
      "source": [
        "### tf.Estimator 생성\n",
        "\n",
        "모델 함수를 만들었다면 tf.estimator를 어떻게 쓰는지 봅시다.\n",
        "\n",
        "간단하게 tf.estimator.Estimator는 모델 함수를 등록하면서 생성하면 됩니다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nRWb4FHZN7wr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "cnn_est = tf.estimator.Estimator(cnn_model_fn)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b_WG19Csb0jr",
        "colab_type": "text"
      },
      "source": [
        "### 모델 학습\n",
        "\n",
        "tf.estimator는 학습과 평가, 예측에 대한 기능을 간편하게 estimator.train(), estimator.evaluate(), estimator.predict()로 실행할 수 있게 지원하고 있습니다.\n",
        "\n",
        "이러한 기능 제공 덕분에 여러분은 모델 학습과 평가를 위한 구현에 대한 고민을 굳이 하실 필요가 없습니다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JbIfpkqbN76N",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "cnn_est.train(train_input_fn)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4PKzXx0Jb2cC",
        "colab_type": "text"
      },
      "source": [
        "### 학습 데이터로 평가점수 확인\n",
        "\n",
        "지금 평가하는 데이터는 학습 데이터입니다.\n",
        "\n",
        "앞에서 loss값은 확인했지만 정확도에 대해서는 확인해보지 못해 여기서 확인해 보도록 합니다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2QY4luLrN8AP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "cnn_est.evaluate(eval_input_fn)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ei-BBq8KcLGZ",
        "colab_type": "text"
      },
      "source": [
        "### 모델 평가\n",
        "\n",
        "이제 실제 평가 데이터를 가지고 성능을 측정해보도록 합시다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ahqu09gMN8Db",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "test_file_link = 'https://raw.githubusercontent.com/NLP-kr/tensorflow-ml-nlp/master/4.TEXT_CLASSIFICATION/data_in/ratings_test.txt'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Pg7Ab60acPUH",
        "colab_type": "text"
      },
      "source": [
        "### 평가 데이터 구성"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N1isq9GWN8Lh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "test_data = pd.read_csv(test_file_link, header = 0, delimiter = '\\t', quoting = 3)\n",
        "\n",
        "clean_test_review = []\n",
        "\n",
        "for review in tqdm(test_data['document']):\n",
        "    # 비어있는 데이터에서 멈추지 않도록 string인 경우만 진행\n",
        "    if type(review) == str:\n",
        "        clean_test_review.append(preprocessing(review, okt, remove_stopwords = True, stop_words=stop_words))\n",
        "    else:\n",
        "        clean_test_review.append([])  #string이 아니면 비어있는 값 추가\n",
        "\n",
        "test_sequences = tokenizer.texts_to_sequences(clean_test_review)\n",
        "test_inputs = pad_sequences(test_sequences, maxlen=MAX_SEQUENCE_LENGTH, padding='post') # 학습 데이터를 벡터화\n",
        "test_labels = np.array(test_data['label']) # 학습 데이터의 라벨"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J4YnppLscTBO",
        "colab_type": "text"
      },
      "source": [
        "### 데이터 입력 함수 "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CcINCwM2N7rq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def test_input_fn():\n",
        "    dataset = tf.data.Dataset.from_tensor_slices((test_inputs, test_labels))\n",
        "    dataset = dataset.batch(128)\n",
        "    dataset = dataset.map(mapping_fn)\n",
        "    iterator = dataset.make_one_shot_iterator()\n",
        "    \n",
        "    return iterator.get_next()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_3Mrj1VzcWDy",
        "colab_type": "text"
      },
      "source": [
        "### CNN 분류기 평가"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nIUDClnwN7mx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "cnn_est.evaluate(test_input_fn)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2JLeUuZ1u6pK",
        "colab_type": "text"
      },
      "source": [
        "## 직접 쓴 문장을 가지고 예측해보기"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nY9HyBJG2-MG",
        "colab_type": "text"
      },
      "source": [
        "### 서빙 함수 구현"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vKmTAJIMu5kG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Serving을 제공하기 위한 입력 리시버 함수를 선언해주어야 한다.\n",
        "def serving_input_receiver_fn():\n",
        "    # estimator에 입력하고자 하는 데이터를 dict 객체로 정의한다.\n",
        "    receiver_tensor = {\n",
        "        # 외부로부터 입력을 받는 프로토콜은 스트링이다. ServingInputReceiver 메뉴얼에도 언급되었다시피\n",
        "        # 이 방식은 TFRecord 파일 형태로 시리얼화한 데이터 형태로 전송을 받는다. (이를 tf.example 방식이라고도 하는 것 같다)\n",
        "        'x': tf.placeholder(dtype=tf.string, shape=[None])\n",
        "    }\n",
        "    \n",
        "    # 다음은 TFRecord 방식으로 받은 데이터를 모델에 넣을 수 있게 처리를 하는 dict 객체라 보면 된다.\n",
        "    # 쉽게 말하면 앞서 estimator를 진행하기 위해 data_fn의 과정을 작성해두는데 이 과정을 여기서 거친다 보면 된다.\n",
        "    features = {\n",
        "        key: tensor\n",
        "        for key, tensor in receiver_tensor.items()\n",
        "    }\n",
        "    # TFRecord로 시리얼화 된 데이터를 integer tensor로 변환하기 위해서는 string to int로 decode를 해줘야 한다.\n",
        "    fn = lambda query: tf.decode_raw(query, tf.int64)\n",
        "    features['x'] = tf.map_fn(fn, features['x'], dtype=tf.int64)\n",
        "    # 받은 데이터에 대해 모델입력에 맞는 shape로 구성을 해주기 위해 reshape을 해준다.\n",
        "    features['x'] = tf.reshape(features['x'], [-1, MAX_SEQUENCE_LENGTH])\n",
        "\n",
        "    # 위에 정의한 받을 데이터에 대한 프로토콜과 모델에 입력할 데이터 전처리를 다음 함수 파라메터에 입력해준다.\n",
        "    return tf.estimator.export.ServingInputReceiver(features, receiver_tensor)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Dc1LDhZ23CC-",
        "colab_type": "text"
      },
      "source": [
        "### 서빙할 모델 파일 생성"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8Utt70LwvZe6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "export_dir_base = './served_model/new_staging'\n",
        "\n",
        "# 서빙에 대한 입력 리시버함수와 저장 위치를 파라메터로 지정한다면, 서빙 pb파일로 저장하여 간단하게 모델을 활용할 수 있게 된다.\n",
        "# 실행을 하게 되면 저장된 파일의 위치를 텍스트 출력을 통해 얻게된다.\n",
        "path = cnn_est.export_savedmodel(export_dir_base, serving_input_receiver_fn)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UOFSVtmd3HH2",
        "colab_type": "text"
      },
      "source": [
        "### 서빙 모델 파일을 예측 모델 함수로 만들기"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GTUT7o8Jv3aD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# 모델이 저장된 경로 위치를 파라메터로 지정하여 함수를 부르면 간단하게 예측 모델을 활용할 수 있다.\n",
        "# 이 모델 예측을 간단하게 함수로 받게된다.\n",
        "predictor_fn = tf.contrib.predictor.from_saved_model(\n",
        "    export_dir = path,\n",
        "    # 옵션이지만 실행되는 모델에 대한 이름을 명시하고자 한다면 다음의 파라메터를 활용한다.\n",
        "    signature_def_key=\"serving_default\"\n",
        ")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OAXrZQPy3Q27",
        "colab_type": "text"
      },
      "source": [
        "### 문장 벡터화"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "njeuHOnyv8LB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def str2vec(s, okt, tokenizer, limit_len=MAX_SEQUENCE_LENGTH):\n",
        "  tokenized_s = preprocessing(s, okt)\n",
        "  sequences = tokenizer.texts_to_sequences([tokenized_s])\n",
        "  sequences = pad_sequences(sequences, maxlen=limit_len, padding='post')\n",
        "\n",
        "  return np.int64(sequences).tostring()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BHWy2wlG3UaF",
        "colab_type": "text"
      },
      "source": [
        "### 예측 테스트"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4sPX3-86xZ29",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "test_sent = '저는 이 영화 완전 추천합니다'\n",
        "model_input = str2vec(test_sent, okt, tokenizer)\n",
        "predicted_value = predictor_fn({'x': [model_input]})\n",
        "print('모델에서 반환된 값: ', predicted_value)\n",
        "print('감정 예측 값 (0~1범위): ',  predicted_value['prob'][0][0])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sfHW947NxZi7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "input_str = input('input text : ')\n",
        "model_input = str2vec(test_sent, okt, tokenizer)\n",
        "predicted_value = predictor_fn({'x': [model_input]})\n",
        "print(predicted_value['prob'][0])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uxBBY_JPOLZ7",
        "colab_type": "text"
      },
      "source": [
        "### Recurrent Neural Network (RNN) 모델\n",
        "\n",
        "![rnn_image](http://tommymullaney.com/img/google-hangouts-feature.png)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eIRc_dDhN7ir",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def rnn_model_fn(features, labels, mode, params):\n",
        "    TRAIN = mode == tf.estimator.ModeKeys.TRAIN\n",
        "    EVAL = mode == tf.estimator.ModeKeys.EVAL\n",
        "    PREDICT = mode == tf.estimator.ModeKeys.PREDICT\n",
        "\n",
        "    embedding_layer = tf.keras.layers.Embedding(\n",
        "                    VOCAB_SIZE,\n",
        "                    EMB_SIZE)(features['x'])\n",
        "\n",
        "    dropout_emb = tf.keras.layers.Dropout(rate = 0.2)(embedding_layer)\n",
        "  \n",
        "    lstm = tf.keras.layers.LSTM(32)(dropout_emb)\n",
        "\n",
        "    hidden = tf.keras.layers.Dense(units=250, activation=tf.nn.relu)(lstm)   \n",
        "\n",
        "\n",
        "    dropout_hidden = tf.keras.layers.Dropout(rate=0.2)(hidden, training = TRAIN)\n",
        "    logits = tf.keras.layers.Dense(units=1)(dropout_hidden)\n",
        "\n",
        "    if labels is not None:\n",
        "        labels = tf.reshape(labels, [-1, 1])\n",
        "        \n",
        "    if TRAIN:\n",
        "        global_step = tf.train.get_global_step()\n",
        "        loss = tf.losses.sigmoid_cross_entropy(labels, logits)\n",
        "        train_op = tf.train.AdamOptimizer(0.001).minimize(loss, global_step)\n",
        "\n",
        "        return tf.estimator.EstimatorSpec(mode=mode, train_op=train_op, loss = loss)\n",
        "    \n",
        "    elif EVAL:\n",
        "        loss = tf.losses.sigmoid_cross_entropy(labels, logits)\n",
        "        pred = tf.nn.sigmoid(logits)\n",
        "        accuracy = tf.metrics.accuracy(labels, tf.round(pred))\n",
        "        return tf.estimator.EstimatorSpec(mode=mode, loss=loss, eval_metric_ops={'acc': accuracy})\n",
        "        \n",
        "    elif PREDICT:\n",
        "        return tf.estimator.EstimatorSpec(\n",
        "            mode=mode,\n",
        "            predictions={\n",
        "                'prob': tf.nn.sigmoid(logits),\n",
        "            }\n",
        "        )"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GKDqTHTOOl7r",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "rnn_est = tf.estimator.Estimator(rnn_model_fn)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9lES3lXCO28B",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "rnn_est.train(train_input_fn)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oCQSOcU5O4_y",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "rnn_est.evaluate(eval_input_fn)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2A8gNJdmPKTF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "rnn_est.evaluate(test_input_fn)"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}